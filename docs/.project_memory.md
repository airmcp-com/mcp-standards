# Project Memory: Simple Personal Memory MCP Server

**Project**: mcp-standards - Simple Personal Memory MCP Server
**Status**: Phase 2 - Enhancement & Documentation
**Version**: 1.0.0 MVP
**Last Updated**: 2025-10-30
**Branch**: fresh-start-simple-memory-mcp

---

## Executive Summary

Building a lightweight, production-ready MCP server that provides Claude with persistent semantic memory for user preferences, corrections, and workflow rules. The server demonstrates MCP best practices with 70-95% auto-discovery rates and serves as a reference implementation for the MCP community.

**üö® CRITICAL UPDATE (2025-10-30)**: Technology stack changed from Python to **Rust or TypeScript**. This resolves the AgentDB compatibility issue - we can now use AgentDB directly as originally specified in PRD!

---

## Critical Findings & Decisions

### 1. AgentDB Technology Mismatch (HIGH PRIORITY)

**Finding**: The PRD specifies AgentDB as the vector database, but research reveals:
- **AgentDB is Node.js-based** (part of ruvnet/agentic-flow npm package)
- **No standalone Python package** exists for AgentDB
- **AgentDB MCP tools** are CLI wrappers executing Node.js commands

**Impact**:
- Cannot directly use AgentDB in Python MCP server as specified in PRD
- Need alternative Python vector database solution

**Options for Resolution**:

**Option A: Python Vector Database Alternatives**
- **ChromaDB**: Popular, native Python, persistent storage, 384-dim support ‚úÖ
- **txtai**: Lightweight, semantic search focused, 100% Python ‚úÖ
- **FAISS**: Facebook's library, extremely fast, requires wrapper for persistence
- **Qdrant**: Production-ready, but heavier (may exceed <100MB target)

**Option B: Hybrid Approach**
- Use AgentDB via subprocess calls to Node.js CLI
- Adds Node.js as dependency (conflicts with Python-only goal)
- Performance overhead from IPC

**Option C: Simple In-Memory Vector Store**
- Implement minimal vector store using numpy/scikit-learn
- sentence-transformers for embeddings
- Pickle for persistence
- Lightest weight, full control

**RECOMMENDATION**: **Option A - ChromaDB** or **Option C - Custom Implementation**
- ChromaDB offers best balance: production-ready, lightweight, native Python
- Custom implementation gives maximum control and minimal dependencies
- Decision needed before Phase 2 completion

---

### 2. Embedding Dimension Correction (CRITICAL)

**Finding**: PRD specifies incorrect embedding dimensions

**PRD States**:
```python
"embedding": List[float],     # 768-dim vector (AgentDB)
```

**Actual Reality**:
- **all-MiniLM-L6-v2 produces 384-dimensional embeddings** (not 768)
- Model specs: 6 layers, 22M parameters, 22MB size
- Output shape: `(batch_size, 384)`

**Correction Required**:
```python
"embedding": List[float],     # 384-dim vector (all-MiniLM-L6-v2)
```

**Impact**:
- Data model in PRD needs update
- Storage calculations affected (half the size = more efficient)
- Memory footprint estimates improve

**Action**: Update PRD Section 4.2 with correct 384-dim specification

---

### 3. MCP Python SDK Selection

**Research Finding**: Multiple Python MCP SDK options exist

**Option A: Official MCP SDK** (`mcp` package)
- Source: `github.com/modelcontextprotocol/python-sdk`
- Pros: Official, full specification support, maintained by Anthropic
- Cons: Lower-level, more boilerplate

**Option B: FastMCP**
- Source: Mentioned in AgentDB implementation docs
- Pros: Decorator-based, less boilerplate, async-first
- Cons: Third-party (may have different maintenance)

**PRD Specifies**: `mcp` package (official SDK)

**RECOMMENDATION**: Start with **official `mcp` SDK** as per PRD
- Aligns with "reference implementation" goal
- Community benefits from official SDK patterns
- Can optimize later if needed

---

## Technology Stack (Confirmed)

### Core Dependencies
```toml
[project]
name = "mcp-standards"
requires-python = ">=3.11"
dependencies = [
    "mcp",                          # Official MCP SDK
    "sentence-transformers",        # Embedding generation (all-MiniLM-L6-v2)
    "chromadb",                     # Vector database (pending decision)
    # OR implement custom vector store
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "mypy>=1.8.0",
    "ruff>=0.2.0",
]
```

### Development Tools
- **Package Manager**: uv (required per global CLAUDE.md)
- **Testing**: pytest with asyncio support
- **Type Checking**: mypy with 100% coverage goal
- **Linting**: ruff (zero errors required)
- **Distribution**: npm package wrapper for `npx mcp-standards`

---

## MCP Tool Auto-Discovery Research

### Formula for 70-95% Auto-Trigger Rates

Based on research and PRD specifications:

```python
Tool(
    name="action_verb",
    description=(
        # 1. What it does (1 sentence, specific domain)
        "<What it does> <domain specificity>. "

        # 2. When to use (directive language)
        "Use when <trigger conditions>. "
        "**IMPORTANT: Use this BEFORE** <prerequisite action>. "

        # 3. Explicit trigger phrases (critical for auto-discovery)
        "**Trigger phrases**: '<phrase1>', '<phrase2>', '<pattern>'. "

        # 4. Concrete examples (real user language)
        "**Examples**: '<concrete example 1>', '<example 2>'. "

        # 5. Always-use scenarios (proactive behavior)
        "**Always use when**: <scenario 1>, <scenario 2>. "

        # 6. Domain context (specific areas)
        "**Categories/Domains**: <specific areas it covers>."
    )
)
```

### Key Patterns That Work

**‚úÖ DO: Explicit trigger phrases**
```
"**Trigger phrases**: 'remember', 'I prefer', 'always use', 'never use'"
```

**‚úÖ DO: Concrete examples in user language**
```
"**Examples**: 'Remember: use uv not pip', 'I prefer conventional commits'"
```

**‚úÖ DO: Directive language for proactive tools**
```
"**IMPORTANT: Use this BEFORE making any tool/command recommendations.**"
```

**‚úÖ DO: Domain-specific context**
```
"Python package managers (pip/uv/poetry/conda)"  # Specific
```

**‚ùå DON'T: Generic descriptions**
```
"Store a preference"  # Too vague, Claude won't know when to use
```

**‚ùå DON'T: Rely on tool names alone**
```
name="remember"  # Description must still be explicit
```

---

## Architecture Decisions

### File Structure (Confirmed from PRD)
```
mcp-standards/
‚îú‚îÄ‚îÄ src/mcp_standards/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ server_simple.py          # Main MCP server entry point
‚îÇ   ‚îú‚îÄ‚îÄ memory_store.py           # Vector database wrapper
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py             # sentence-transformers wrapper
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Helpers (validation, etc.)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_server.py            # MCP server integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_memory_store.py      # Vector DB unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_embeddings.py        # Embedding generation tests
‚îÇ   ‚îî‚îÄ‚îÄ test_integration.py       # End-to-end workflows
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ .project_memory.md        # This file
‚îÇ   ‚îú‚îÄ‚îÄ PRD.md                    # Product requirements
‚îÇ   ‚îú‚îÄ‚îÄ SETUP_GUIDE.md            # Installation instructions
‚îÇ   ‚îî‚îÄ‚îÄ VALIDATION_CHECKLIST.md   # Testing checklist
‚îú‚îÄ‚îÄ pyproject.toml                # Python package config
‚îú‚îÄ‚îÄ package.json                  # npm wrapper config
‚îú‚îÄ‚îÄ CLAUDE.md                     # Project-specific config
‚îî‚îÄ‚îÄ README.md                     # User-facing documentation
```

### Module Execution Pattern (Critical)

**ALWAYS use module syntax to avoid ImportError**:

```bash
# ‚ùå WRONG (breaks relative imports)
python src/mcp_standards/server_simple.py

# ‚úÖ CORRECT (preserves package context)
python -m mcp_standards.server_simple
```

**Why**: Python packages with `__init__.py` require module context for relative imports

**Claude Desktop Config**:
```json
{
  "mcpServers": {
    "memory": {
      "command": "uv",
      "args": [
        "run",
        "--directory",
        "/absolute/path/to/mcp-standards",
        "python",
        "-m",
        "mcp_standards.server_simple"
      ]
    }
  }
}
```

---

## Performance Targets (From PRD)

### Success Metrics
- **Auto-trigger Rate**: 70-95% for implicit preference checks
- **Setup Time**: <5 minutes from install to first use
- **Memory Footprint**: <100MB for typical usage (1000+ preferences)
- **Query Latency**: <100ms for semantic search (p99)
- **Embedding Generation**: <50ms per text
- **Startup Time**: <2 seconds for server initialization

### Testing Requirements
- **Test Coverage**: >80% (pytest-cov)
- **Type Coverage**: 100% (mypy strict mode)
- **Linting**: Zero errors (ruff)
- **Integration Tests**: All PRD workflows covered

---

## Data Model (Corrected)

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Optional

@dataclass
class MemoryRecord:
    """Single user preference/correction stored in memory."""

    id: str                           # UUID4 identifier
    content: str                      # User preference text
    category: str                     # python|git|docker|general
    importance: int                   # 1-10 priority score
    timestamp: datetime               # Creation time (UTC)
    embedding: List[float]            # 384-dim vector (all-MiniLM-L6-v2) ‚úÖ CORRECTED
    metadata: Dict[str, str]          # Additional context

    # Metadata fields:
    # - "source": "user_correction" | "explicit"
    # - "context": Optional context string
```

---

## Vector Database Implementation Options

### Option A: ChromaDB (Recommended)

**Pros**:
- Native Python, async support
- Persistent storage with SQLite backend
- Built-in embedding function support
- Lightweight (<50MB typical usage)
- Active development, production-ready

**Example Implementation**:
```python
import chromadb
from chromadb.config import Settings

client = chromadb.Client(Settings(
    persist_directory="~/.mcp-memory",
    anonymized_telemetry=False
))

collection = client.get_or_create_collection(
    name="user_preferences",
    metadata={"hnsw:space": "cosine"}
)

# Store preference
collection.add(
    embeddings=[embedding],
    documents=[content],
    metadatas=[{"category": "python", "importance": 8}],
    ids=[uuid4().hex]
)

# Query preferences
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5,
    where={"category": "python"}  # Optional filter
)
```

**Performance**: Meets <100ms target for typical queries

---

### Option B: Custom Vector Store

**Pros**:
- Zero external dependencies (beyond numpy/sklearn)
- Full control over implementation
- Minimal memory footprint
- Educational value for reference implementation

**Example Implementation**:
```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle

class SimpleVectorStore:
    def __init__(self, persist_path: str):
        self.memories: List[MemoryRecord] = []
        self.embeddings: np.ndarray = None
        self.persist_path = persist_path
        self.load()

    def add(self, memory: MemoryRecord):
        self.memories.append(memory)
        self._rebuild_index()
        self.save()

    def search(self, query_embedding: List[float], limit: int = 5) -> List[tuple]:
        if len(self.memories) == 0:
            return []

        query_vec = np.array(query_embedding).reshape(1, -1)
        similarities = cosine_similarity(query_vec, self.embeddings)[0]

        # Get top k indices
        top_indices = np.argsort(similarities)[-limit:][::-1]

        return [(self.memories[i], similarities[i]) for i in top_indices]

    def _rebuild_index(self):
        self.embeddings = np.array([m.embedding for m in self.memories])

    def save(self):
        with open(self.persist_path, 'wb') as f:
            pickle.dump(self.memories, f)

    def load(self):
        if os.path.exists(self.persist_path):
            with open(self.persist_path, 'rb') as f:
                self.memories = pickle.load(f)
            self._rebuild_index()
```

**Performance**: Expected <50ms for small datasets (<1000 items)

---

### Option C: txtai (Middle Ground)

**Pros**:
- Lightweight semantic search library
- Built specifically for semantic search use cases
- Simpler API than ChromaDB
- Good balance of features and complexity

**Example Implementation**:
```python
from txtai.embeddings import Embeddings

embeddings = Embeddings({
    "path": "sentence-transformers/all-MiniLM-L6-v2",
    "content": True
})

# Index preferences
embeddings.index([
    (id, content, metadata)
    for id, content, metadata in preferences
])

# Search
results = embeddings.search("python package manager", limit=5)
```

---

## MCP Tool Definitions (Draft)

### remember Tool
```python
@server.tool()
async def remember(
    content: str,
    category: str = "general",
    importance: int = 5
) -> dict:
    """
    Store user preferences, corrections, and workflow rules in semantic memory.
    Use when user explicitly shares preferences or corrects your suggestions.

    **Trigger phrases**: 'remember', 'I prefer', 'always use', 'never use',
    'my workflow', 'instead of', 'not X, use Y', 'actually use X'.

    **Examples**: 'Remember: use uv not pip', 'I prefer conventional commits',
    'Always create feature branches', 'Never commit to main directly'.

    **Categories**: python (pip/uv/poetry/conda), git (commit/branch/merge),
    docker (compose/build), general (any other preferences).

    Args:
        content: The preference text to remember
        category: Domain classification (python|git|docker|general)
        importance: Priority score from 1-10 (default 5)

    Returns:
        {"id": str, "message": str, "content": str}
    """
    # Implementation: Generate embedding, store in vector DB, return confirmation
    pass
```

### recall Tool
```python
@server.tool()
async def recall(
    query: str,
    category: Optional[str] = None,
    limit: int = 5
) -> list:
    """
    Search user's stored preferences using semantic search.

    **IMPORTANT: Use this BEFORE making any tool/command recommendations.**

    Check if user has preferences for: Python package managers (pip/uv/poetry/conda),
    Git workflows (commit styles, branch naming, merge vs rebase),
    Docker usage (compose/CLI), build tools (npm/yarn/pnpm), testing frameworks.

    **Always use when**: suggesting commands, recommending tools, before running
    npm/pip/git commands, when user asks 'how do I...' questions.

    **Examples**: 'python package manager', 'git commit style',
    'preferred testing framework', 'docker workflow'.

    Args:
        query: Search query for preferences
        category: Optional filter by domain
        limit: Maximum results to return (default 5)

    Returns:
        [{"content": str, "category": str, "importance": int, "score": float}]
    """
    # Implementation: Generate query embedding, search vector DB, return ranked results
    pass
```

---

## Validation Checklist Integration

### Installation Validation
- [ ] `uv pip install -e .` succeeds without errors
- [ ] `python -m mcp_standards.server_simple` starts server
- [ ] Server appears in Claude Desktop MCP tools list
- [ ] No import errors or missing dependencies

### Functional Validation
- [ ] `remember` tool accepts and stores preferences
- [ ] `recall` tool retrieves relevant preferences
- [ ] Semantic search returns accurate results (>0.7 similarity)
- [ ] Edge cases handled gracefully (empty queries, no results)

### Performance Validation
- [ ] Query latency <100ms (p99 with 100+ memories)
- [ ] Memory footprint <100MB (with 1000 memories)
- [ ] Startup time <2 seconds
- [ ] Embedding generation <50ms per text

### Auto-Discovery Validation
- [ ] `remember` auto-triggers on "Remember: use X" (>85% rate)
- [ ] `recall` auto-triggers before tool recommendations (>70% rate)
- [ ] Tools appear in Claude's available tools list
- [ ] Tool descriptions render correctly in UI

---

## Open Questions & Decisions Needed

### 1. Vector Database Choice (BLOCKING)
**Question**: Which vector database should we use?
**Options**: ChromaDB, txtai, custom implementation
**Recommendation**: ChromaDB for production-ready MVP, custom for educational value
**Decision Owner**: Matt Strautmann
**Timeline**: Before starting Phase 2 implementation

### 2. PRD Updates Required
**Question**: Should we update PRD with corrected specs?
**Changes Needed**:
- Embedding dimensions: 768 ‚Üí 384
- AgentDB clarification: Node.js vs Python alternatives
- Vector DB options: Document ChromaDB/txtai alternatives
**Recommendation**: Yes, update PRD to reflect research findings
**Timeline**: During Phase 2 documentation work

### 3. npm Package Distribution
**Question**: How should we structure the npm wrapper?
**Purpose**: Enable `npx mcp-standards install` command
**Requirement**: Auto-configure Claude Desktop config
**Timeline**: Phase 3 (production hardening)

---

## Success Criteria Tracking

### Phase 2 Goals (Current)
- [x] PRD document complete
- [x] Research AgentDB and alternatives
- [x] Document MCP best practices
- [ ] Create comprehensive project memory (this file)
- [ ] Update CLAUDE.md with project context
- [ ] Resolve vector database decision
- [ ] Update PRD with corrected specifications
- [ ] Complete validation checklist refinement

### MVP Completion Criteria
- [ ] Core tools implemented (`remember`, `recall`, `list_memories`)
- [ ] Test coverage >80%
- [ ] Type checking 100% (mypy strict)
- [ ] Auto-discovery rate >70% (measured)
- [ ] Performance targets met (<100ms queries)
- [ ] Documentation complete (setup guide, validation)
- [ ] Ready for Phase 3 production hardening

---

## References & Resources

### Documentation
- [MCP Specification](https://modelcontextprotocol.io/specification/2025-06-18/server/tools)
- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)
- [sentence-transformers Documentation](https://www.sbert.net/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [txtai Documentation](https://neuml.github.io/txtai/)

### Related Projects
- [ruvnet/agentic-flow](https://github.com/ruvnet/agentic-flow) - AgentDB source
- [ruvnet/claude-flow](https://github.com/ruvnet/claude-flow) - Orchestration platform

### Internal Documents
- [PRD.md](PRD.md) - Product requirements (needs updates)
- [CLAUDE.md](CLAUDE.md) - Project configuration (needs project context)
- [VALIDATION_CHECKLIST.md](VALIDATION_CHECKLIST.md) - Testing procedures (to be refined)

---

## Change Log

### 2025-10-30
- **Created**: Initial project memory document
- **Research**: Completed AgentDB, MCP SDK, embedding model analysis
- **Findings**: Documented technology mismatch and dimension correction
- **Decisions**: Pending vector database choice
- **Next Steps**: Update CLAUDE.md, resolve blocking decisions

---

**Document Status**: Living document, updated as project progresses
**Maintainer**: Claude Code with human oversight
**Review Cadence**: After each major milestone or decision point
